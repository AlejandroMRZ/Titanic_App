{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como titanic_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(data, is_train=True):\n",
    "    # Select relevant columns for training/testing\n",
    "    columns = [\"Pclass\", \"Sex\", \"Age\"]\n",
    "    if is_train:\n",
    "        columns.append(\"Survived\")  # Include target only for training\n",
    "    \n",
    "    data = data[columns].dropna()\n",
    "    data[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\": 1})  # Encode 'Sex'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data = preprocess(train_data, is_train=True)\n",
    "test_data = preprocess(test_data, is_train=False)\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_data[[\"Pclass\", \"Sex\", \"Age\"]]\n",
    "y_train = train_data[\"Survived\"]\n",
    "\n",
    "X_test = test_data[[\"Pclass\", \"Sex\", \"Age\"]]\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# (Optional) Predict on test data\n",
    "# Since `test.csv` has no 'Survived' column, predictions can't be evaluated but can be saved\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"titanic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Modelo entrenado y guardado como titanic_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key Loaded: AIzaS*****\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file from the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key\n",
    "GOOGLE_AI_STUDIO = os.getenv(\"GOOGLE_AI_STUDIO\")\n",
    "\n",
    "# Debugging output\n",
    "if GOOGLE_AI_STUDIO:\n",
    "    print(\"âœ… API Key Loaded:\", GOOGLE_AI_STUDIO[:5] + \"*****\")\n",
    "else:\n",
    "    print(\"âŒ ERROR: API key not loaded. Check the .env file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key Loaded: AIzaS*****\n",
      "\n",
      "Texto Generado:\n",
      " Absolutamente. Como historiador experto en el Titanic, analizarÃ© la supervivencia de este pasajero basÃ¡ndome en los datos proporcionados y el contexto histÃ³rico del desastre.\n",
      "\n",
      "**AnÃ¡lisis del pasajero:**\n",
      "\n",
      "**Clase:** {pclass}\n",
      "**GÃ©nero:** {sex}\n",
      "**Edad:** {age} aÃ±os\n",
      "**PredicciÃ³n del modelo:** {prediction}\n",
      "\n",
      "**ExplicaciÃ³n HistÃ³rica y LÃ³gica:**\n",
      "\n",
      "La supervivencia en el Titanic no fue un evento aleatorio, sino que estuvo fuertemente influenciada por factores como la clase social, el gÃ©nero y, en menor medida, la edad. El modelo de Machine Learning, al igual que la historia, refleja estas tendencias.\n",
      "\n",
      "*   **Clase Social y Acceso a Botes Salvavidas:** La clase en la que viajaba un pasajero era un predictor crucial de su supervivencia. \n",
      "    *   **Primera Clase:** Los pasajeros de primera clase tenÃ­an camarotes ubicados en las cubiertas superiores, mÃ¡s cerca de los botes salvavidas. AdemÃ¡s, gozaban de mayor privilegio y prioridad al momento de abordar estos botes. La tripulaciÃ³n tambiÃ©n tenÃ­a la instrucciÃ³n de priorizar a los pasajeros de primera clase.\n",
      "    *   **Segunda Clase:** Los pasajeros de segunda clase tenÃ­an una ubicaciÃ³n intermedia y, aunque tuvieron acceso a los botes, no con la misma prioridad que la primera clase.\n",
      "    *   **Tercera Clase:** Los pasajeros de tercera clase estaban ubicados en las cubiertas inferiores, mÃ¡s lejos de los botes y a menudo con acceso restringido a las cubiertas superiores. La tripulaciÃ³n inicialmente priorizÃ³ la evacuaciÃ³n de las clases superiores, lo que resultÃ³ en una tasa de supervivencia mucho menor para los pasajeros de tercera clase.\n",
      "\n",
      "*   **GÃ©nero: \"Mujeres y NiÃ±os Primero\":** La consigna \"mujeres y niÃ±os primero\" fue un factor determinante en la supervivencia. Aunque no fue aplicada al 100% y hubo excepciones, la prioridad dada a mujeres y niÃ±os resultÃ³ en una mayor tasa de supervivencia para este grupo demogrÃ¡fico.\n",
      "\n",
      "*   **Edad:** Si bien la edad tuvo un impacto menor en comparaciÃ³n con la clase y el gÃ©nero, los niÃ±os, especialmente los mÃ¡s pequeÃ±os, tuvieron una mayor probabilidad de ser rescatados.\n",
      "\n",
      "**AnÃ¡lisis EspecÃ­fico del Pasajero:**\n",
      "\n",
      "Ahora, analicemos al pasajero con los datos proporcionados:\n",
      "\n",
      "*   **Si el pasajero es de Primera Clase, mujer y/o niÃ±o:** La predicciÃ³n de supervivencia serÃ­a **alta**. La combinaciÃ³n de clase alta y gÃ©nero femenino o infantil aumentaba significativamente sus posibilidades de abordar un bote salvavidas.\n",
      "*   **Si el pasajero es de Segunda Clase, mujer y/o niÃ±o:** La predicciÃ³n de supervivencia serÃ­a **moderada**. TenÃ­an acceso a los botes, pero no con la misma prioridad que la primera clase.\n",
      "*   **Si el pasajero es de Tercera Clase, mujer y/o niÃ±o:** La predicciÃ³n de supervivencia serÃ­a **menor**. Aunque se priorizaron mujeres y niÃ±os, la ubicaciÃ³n de la tercera clase dificultÃ³ su acceso a los botes salvavidas.\n",
      "*   **Si el pasajero es un hombre de cualquier clase:** La predicciÃ³n de supervivencia serÃ­a **baja**. Los hombres tuvieron una tasa de supervivencia mucho menor en comparaciÃ³n con las mujeres y los niÃ±os, independientemente de su clase.\n",
      "\n",
      "**ConclusiÃ³n:**\n",
      "\n",
      "La predicciÃ³n del modelo {prediction} concuerda con las tendencias histÃ³ricas del desastre del Titanic. La clase social, el gÃ©nero y la edad fueron factores determinantes en la supervivencia, y el modelo de Machine Learning ha logrado capturar estas correlaciones de manera efectiva. El anÃ¡lisis de los datos del pasajero, a la luz de los hechos histÃ³ricos, nos permite entender por quÃ© este pasajero habrÃ­a sobrevivido o no al desastre del Titanic.\n",
      "\n",
      "Espero que este anÃ¡lisis detallado y lÃ³gico sea de utilidad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_AI_STUDIO\")\n",
    "\n",
    "# Verificar que se haya cargado la API key\n",
    "if not api_key:\n",
    "    print(\"âŒ API Key NOT Loaded. Please check your .env file.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"âœ… API Key Loaded:\", api_key[:5] + \"*****\")\n",
    "\n",
    "# Configurar la API key de Google AI\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Definir el modelo a utilizar\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-exp') # O 'gemini-pro' para el modelo pro\n",
    "\n",
    "\n",
    "# FunciÃ³n para generar texto\n",
    "def generate_text(prompt,\n",
    "                 temperature=0.9,\n",
    "                 top_p=1.0,\n",
    "                 top_k=40,\n",
    "                 max_output_tokens=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Genera texto utilizando el modelo Gemini.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): El texto inicial o prompt para guiar la generaciÃ³n.\n",
    "        temperature (float, optional): Controla la aleatoriedad de la generaciÃ³n. \n",
    "            Valores mÃ¡s altos (e.g., 1.0) hacen que el texto sea mÃ¡s diverso e impredecible.\n",
    "            Valores mÃ¡s bajos (e.g., 0.2) hacen que el texto sea mÃ¡s determinista. \n",
    "            Por defecto es 0.9.\n",
    "        top_p (float, optional): El umbral para la selecciÃ³n de tokens. \n",
    "            Se eligen tokens con una probabilidad acumulada menor o igual a top_p.\n",
    "            Por defecto es 1.0 (selecciona todos los tokens posibles).\n",
    "        top_k (int, optional): El nÃºmero mÃ¡ximo de tokens a considerar para la selecciÃ³n en cada paso. \n",
    "            Por defecto es 40.\n",
    "        max_output_tokens (int, optional): El nÃºmero mÃ¡ximo de tokens que se generarÃ¡n.\n",
    "            Si es None, el modelo usarÃ¡ un lÃ­mite predeterminado.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto generado por el modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "         response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "               temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                max_output_tokens=max_output_tokens\n",
    "            )\n",
    "        )\n",
    "         return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error during text generation: {e}\"\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == '__main__':\n",
    "    prompt_text = (\n",
    "    \"Eres un historiador experto en el Titanic y tienes acceso a un modelo de Machine Learning que predice la supervivencia de los pasajeros. \"\n",
    "    \"Utilizando las siguientes caracterÃ­sticas, explica de forma lÃ³gica y detallada por quÃ© este pasajero habrÃ­a sobrevivido o no:\\n\\n\"\n",
    "    \"ðŸ›³ï¸ **Datos del pasajero:**\\n\"\n",
    "    \"- Clase: {pclass}\\n\"\n",
    "    \"- GÃ©nero: {sex}\\n\"\n",
    "    \"- Edad: {age} aÃ±os\\n\"\n",
    "    \"- PredicciÃ³n del modelo: {prediction}\\n\\n\"\n",
    "    \"ðŸ“– **Instrucciones:**\\n\"\n",
    "    \"- Proporciona un anÃ¡lisis histÃ³rico y lÃ³gico basado en estos datos.\\n\"\n",
    "    \"- Relaciona la clase social y el acceso a los botes salvavidas.\\n\"\n",
    "    \"- SÃ© claro, conciso y profesional.\\n\\n\"\n",
    "    \"ðŸ“¢ **ExplicaciÃ³n:**\"\n",
    ")\n",
    "    generated_text = generate_text(prompt_text, temperature = 0.5)\n",
    "    print(\"\\nTexto Generado:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.159.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (5.29.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (2.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from pydantic->google-generativeai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio-1.69.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Using cached google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "Using cached google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading google_api_python_client-2.159.0-py2.py3-none-any.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.0/12.8 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 30.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 28.7 MB/s eta 0:00:00\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.69.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.69.0-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.4/4.4 MB 29.4 MB/s eta 0:00:00\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, proto-plus, httplib2, grpcio, googleapis-common-protos, rsa, pyasn1-modules, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.1\n",
      "    Uninstalling grpcio-1.68.1:\n",
      "      Successfully uninstalled grpcio-1.68.1\n",
      "Successfully installed google-ai-generativelanguage-0.6.10 google-api-core-2.24.0 google-api-python-client-2.159.0 google-auth-2.37.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.3 googleapis-common-protos-1.66.0 grpcio-1.69.0 grpcio-status-1.69.0 httplib2-0.22.0 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
