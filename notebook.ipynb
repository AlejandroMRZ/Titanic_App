{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como titanic_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(data, is_train=True):\n",
    "    # Select relevant columns for training/testing\n",
    "    columns = [\"Pclass\", \"Sex\", \"Age\"]\n",
    "    if is_train:\n",
    "        columns.append(\"Survived\")  # Include target only for training\n",
    "    \n",
    "    data = data[columns].dropna()\n",
    "    data[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\": 1})  # Encode 'Sex'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data = preprocess(train_data, is_train=True)\n",
    "test_data = preprocess(test_data, is_train=False)\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_data[[\"Pclass\", \"Sex\", \"Age\"]]\n",
    "y_train = train_data[\"Survived\"]\n",
    "\n",
    "X_test = test_data[[\"Pclass\", \"Sex\", \"Age\"]]\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# (Optional) Predict on test data\n",
    "# Since `test.csv` has no 'Survived' column, predictions can't be evaluated but can be saved\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"titanic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Modelo entrenado y guardado como titanic_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key Loaded: AIzaS*****\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file from the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key\n",
    "GOOGLE_AI_STUDIO = os.getenv(\"GOOGLE_AI_STUDIO\")\n",
    "\n",
    "# Debugging output\n",
    "if GOOGLE_AI_STUDIO:\n",
    "    print(\"✅ API Key Loaded:\", GOOGLE_AI_STUDIO[:5] + \"*****\")\n",
    "else:\n",
    "    print(\"❌ ERROR: API key not loaded. Check the .env file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key Loaded: AIzaS*****\n",
      "\n",
      "Texto Generado:\n",
      " Absolutamente. Como historiador experto en el Titanic, analizaré la supervivencia de este pasajero basándome en los datos proporcionados y el contexto histórico del desastre.\n",
      "\n",
      "**Análisis del pasajero:**\n",
      "\n",
      "**Clase:** {pclass}\n",
      "**Género:** {sex}\n",
      "**Edad:** {age} años\n",
      "**Predicción del modelo:** {prediction}\n",
      "\n",
      "**Explicación Histórica y Lógica:**\n",
      "\n",
      "La supervivencia en el Titanic no fue un evento aleatorio, sino que estuvo fuertemente influenciada por factores como la clase social, el género y, en menor medida, la edad. El modelo de Machine Learning, al igual que la historia, refleja estas tendencias.\n",
      "\n",
      "*   **Clase Social y Acceso a Botes Salvavidas:** La clase en la que viajaba un pasajero era un predictor crucial de su supervivencia. \n",
      "    *   **Primera Clase:** Los pasajeros de primera clase tenían camarotes ubicados en las cubiertas superiores, más cerca de los botes salvavidas. Además, gozaban de mayor privilegio y prioridad al momento de abordar estos botes. La tripulación también tenía la instrucción de priorizar a los pasajeros de primera clase.\n",
      "    *   **Segunda Clase:** Los pasajeros de segunda clase tenían una ubicación intermedia y, aunque tuvieron acceso a los botes, no con la misma prioridad que la primera clase.\n",
      "    *   **Tercera Clase:** Los pasajeros de tercera clase estaban ubicados en las cubiertas inferiores, más lejos de los botes y a menudo con acceso restringido a las cubiertas superiores. La tripulación inicialmente priorizó la evacuación de las clases superiores, lo que resultó en una tasa de supervivencia mucho menor para los pasajeros de tercera clase.\n",
      "\n",
      "*   **Género: \"Mujeres y Niños Primero\":** La consigna \"mujeres y niños primero\" fue un factor determinante en la supervivencia. Aunque no fue aplicada al 100% y hubo excepciones, la prioridad dada a mujeres y niños resultó en una mayor tasa de supervivencia para este grupo demográfico.\n",
      "\n",
      "*   **Edad:** Si bien la edad tuvo un impacto menor en comparación con la clase y el género, los niños, especialmente los más pequeños, tuvieron una mayor probabilidad de ser rescatados.\n",
      "\n",
      "**Análisis Específico del Pasajero:**\n",
      "\n",
      "Ahora, analicemos al pasajero con los datos proporcionados:\n",
      "\n",
      "*   **Si el pasajero es de Primera Clase, mujer y/o niño:** La predicción de supervivencia sería **alta**. La combinación de clase alta y género femenino o infantil aumentaba significativamente sus posibilidades de abordar un bote salvavidas.\n",
      "*   **Si el pasajero es de Segunda Clase, mujer y/o niño:** La predicción de supervivencia sería **moderada**. Tenían acceso a los botes, pero no con la misma prioridad que la primera clase.\n",
      "*   **Si el pasajero es de Tercera Clase, mujer y/o niño:** La predicción de supervivencia sería **menor**. Aunque se priorizaron mujeres y niños, la ubicación de la tercera clase dificultó su acceso a los botes salvavidas.\n",
      "*   **Si el pasajero es un hombre de cualquier clase:** La predicción de supervivencia sería **baja**. Los hombres tuvieron una tasa de supervivencia mucho menor en comparación con las mujeres y los niños, independientemente de su clase.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "La predicción del modelo {prediction} concuerda con las tendencias históricas del desastre del Titanic. La clase social, el género y la edad fueron factores determinantes en la supervivencia, y el modelo de Machine Learning ha logrado capturar estas correlaciones de manera efectiva. El análisis de los datos del pasajero, a la luz de los hechos históricos, nos permite entender por qué este pasajero habría sobrevivido o no al desastre del Titanic.\n",
      "\n",
      "Espero que este análisis detallado y lógico sea de utilidad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_AI_STUDIO\")\n",
    "\n",
    "# Verificar que se haya cargado la API key\n",
    "if not api_key:\n",
    "    print(\"❌ API Key NOT Loaded. Please check your .env file.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"✅ API Key Loaded:\", api_key[:5] + \"*****\")\n",
    "\n",
    "# Configurar la API key de Google AI\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Definir el modelo a utilizar\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-exp') # O 'gemini-pro' para el modelo pro\n",
    "\n",
    "\n",
    "# Función para generar texto\n",
    "def generate_text(prompt,\n",
    "                 temperature=0.9,\n",
    "                 top_p=1.0,\n",
    "                 top_k=40,\n",
    "                 max_output_tokens=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Genera texto utilizando el modelo Gemini.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): El texto inicial o prompt para guiar la generación.\n",
    "        temperature (float, optional): Controla la aleatoriedad de la generación. \n",
    "            Valores más altos (e.g., 1.0) hacen que el texto sea más diverso e impredecible.\n",
    "            Valores más bajos (e.g., 0.2) hacen que el texto sea más determinista. \n",
    "            Por defecto es 0.9.\n",
    "        top_p (float, optional): El umbral para la selección de tokens. \n",
    "            Se eligen tokens con una probabilidad acumulada menor o igual a top_p.\n",
    "            Por defecto es 1.0 (selecciona todos los tokens posibles).\n",
    "        top_k (int, optional): El número máximo de tokens a considerar para la selección en cada paso. \n",
    "            Por defecto es 40.\n",
    "        max_output_tokens (int, optional): El número máximo de tokens que se generarán.\n",
    "            Si es None, el modelo usará un límite predeterminado.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto generado por el modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "         response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "               temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                max_output_tokens=max_output_tokens\n",
    "            )\n",
    "        )\n",
    "         return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error during text generation: {e}\"\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == '__main__':\n",
    "    prompt_text = (\n",
    "    \"Eres un historiador experto en el Titanic y tienes acceso a un modelo de Machine Learning que predice la supervivencia de los pasajeros. \"\n",
    "    \"Utilizando las siguientes características, explica de forma lógica y detallada por qué este pasajero habría sobrevivido o no:\\n\\n\"\n",
    "    \"🛳️ **Datos del pasajero:**\\n\"\n",
    "    \"- Clase: {pclass}\\n\"\n",
    "    \"- Género: {sex}\\n\"\n",
    "    \"- Edad: {age} años\\n\"\n",
    "    \"- Predicción del modelo: {prediction}\\n\\n\"\n",
    "    \"📖 **Instrucciones:**\\n\"\n",
    "    \"- Proporciona un análisis histórico y lógico basado en estos datos.\\n\"\n",
    "    \"- Relaciona la clase social y el acceso a los botes salvavidas.\\n\"\n",
    "    \"- Sé claro, conciso y profesional.\\n\\n\"\n",
    "    \"📢 **Explicación:**\"\n",
    ")\n",
    "    generated_text = generate_text(prompt_text, temperature = 0.5)\n",
    "    print(\"\\nTexto Generado:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.159.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (5.29.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (2.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from pydantic->google-generativeai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex_\\miniconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio-1.69.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Using cached google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "Using cached google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading google_api_python_client-2.159.0-py2.py3-none-any.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.0/12.8 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 30.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 28.7 MB/s eta 0:00:00\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.69.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.69.0-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.4/4.4 MB 29.4 MB/s eta 0:00:00\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, proto-plus, httplib2, grpcio, googleapis-common-protos, rsa, pyasn1-modules, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.1\n",
      "    Uninstalling grpcio-1.68.1:\n",
      "      Successfully uninstalled grpcio-1.68.1\n",
      "Successfully installed google-ai-generativelanguage-0.6.10 google-api-core-2.24.0 google-api-python-client-2.159.0 google-auth-2.37.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.3 googleapis-common-protos-1.66.0 grpcio-1.69.0 grpcio-status-1.69.0 httplib2-0.22.0 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
